diff --git a/drivers/gpu/drm/i915/Makefile b/drivers/gpu/drm/i915/Makefile
index 5794f10..07a619c 100644
--- a/drivers/gpu/drm/i915/Makefile
+++ b/drivers/gpu/drm/i915/Makefile
@@ -183,7 +183,8 @@ i915-y += i915_perf.o \
 	  i915_oa_icl.o
 
 ifeq ($(CONFIG_DRM_I915_GVT),y)
-i915-y += intel_gvt.o
+i915-y += intel_gvt.o \
+	  i915_gem_vgtbuffer.o
 include $(src)/gvt/Makefile
 endif
 
diff --git a/drivers/gpu/drm/i915/gvt/hypercall.h b/drivers/gpu/drm/i915/gvt/hypercall.h
index ae9b94a..715c85a 100644
--- a/drivers/gpu/drm/i915/gvt/hypercall.h
+++ b/drivers/gpu/drm/i915/gvt/hypercall.h
@@ -63,6 +63,7 @@ struct intel_gvt_mpt {
 	int (*get_vfio_device)(void *vgpu);
 	void (*put_vfio_device)(void *vgpu);
 	bool (*is_valid_gfn)(unsigned long handle, unsigned long gfn);
+	struct intel_vgpu *(*vgpu_from_vm_id)(int vm_id);
 };
 
 extern struct intel_gvt_mpt xengt_mpt;
diff --git a/drivers/gpu/drm/i915/gvt/mpt.h b/drivers/gpu/drm/i915/gvt/mpt.h
index d16bc40..898b1bc 100644
--- a/drivers/gpu/drm/i915/gvt/mpt.h
+++ b/drivers/gpu/drm/i915/gvt/mpt.h
@@ -347,4 +347,19 @@ static inline bool intel_gvt_hypervisor_is_valid_gfn(
 	return intel_gvt_host.mpt->is_valid_gfn(vgpu->handle, gfn);
 }
 
+/**
+ * intel_gvt_hypervisor_vgpu_from_vm_id - translate external VM id into vgpu
+ * @vm_id: a VM id
+ *
+ * Returns:
+ * vgpu pointer on success, NULL if failed
+ */
+static inline struct intel_vgpu *intel_gvt_hypervisor_vgpu_from_vm_id(int vm_id)
+{
+	if (!intel_gvt_host.mpt->vgpu_from_vm_id)
+		return NULL;
+
+	return intel_gvt_host.mpt->vgpu_from_vm_id(vm_id);
+}
+
 #endif /* _GVT_MPT_H_ */
diff --git a/drivers/gpu/drm/i915/gvt/xengt.c b/drivers/gpu/drm/i915/gvt/xengt.c
index 6fbbf2d..8b4da45 100644
--- a/drivers/gpu/drm/i915/gvt/xengt.c
+++ b/drivers/gpu/drm/i915/gvt/xengt.c
@@ -1998,6 +1998,7 @@ struct intel_gvt_mpt xengt_mpt = {
 	.dma_unmap_guest_page = xengt_dma_unmap_guest_page,
 	.map_gfn_to_mfn = xengt_map_gfn_to_mfn,
 	.set_trap_area = xengt_set_trap_area,
+	.vgpu_from_vm_id = vgpu_from_vm_id,
 };
 EXPORT_SYMBOL_GPL(xengt_mpt);
 
diff --git a/drivers/gpu/drm/i915/i915_drv.c b/drivers/gpu/drm/i915/i915_drv.c
index f8cfd16..fda068c 100644
--- a/drivers/gpu/drm/i915/i915_drv.c
+++ b/drivers/gpu/drm/i915/i915_drv.c
@@ -2860,6 +2860,7 @@ static const struct drm_ioctl_desc i915_ioctls[] = {
 	DRM_IOCTL_DEF_DRV(I915_PERF_ADD_CONFIG, i915_perf_add_config_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_PERF_REMOVE_CONFIG, i915_perf_remove_config_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
 	DRM_IOCTL_DEF_DRV(I915_QUERY, i915_query_ioctl, DRM_UNLOCKED|DRM_RENDER_ALLOW),
+	DRM_IOCTL_DEF_DRV(I915_GEM_VGTBUFFER, i915_gem_vgtbuffer_ioctl, DRM_RENDER_ALLOW),
 };
 
 static struct drm_driver driver = {
diff --git a/drivers/gpu/drm/i915/i915_drv.h b/drivers/gpu/drm/i915/i915_drv.h
index d6c25bea..e755419 100644
--- a/drivers/gpu/drm/i915/i915_drv.h
+++ b/drivers/gpu/drm/i915/i915_drv.h
@@ -2874,6 +2874,15 @@ int i915_gem_set_tiling_ioctl(struct drm_device *dev, void *data,
 			      struct drm_file *file_priv);
 int i915_gem_get_tiling_ioctl(struct drm_device *dev, void *data,
 			      struct drm_file *file_priv);
+#ifdef CONFIG_DRM_I915_GVT
+int i915_gem_vgtbuffer_ioctl(struct drm_device *dev, void *data,
+                             struct drm_file *file);
+#else
+static inline
+int i915_gem_vgtbuffer_ioctl(struct drm_device *dev, void *data,
+                             struct drm_file *file)
+{ return -ENOTTY; }
+#endif
 int i915_gem_init_userptr(struct drm_i915_private *dev_priv);
 void i915_gem_cleanup_userptr(struct drm_i915_private *dev_priv);
 int i915_gem_userptr_ioctl(struct drm_device *dev, void *data,
diff --git a/drivers/gpu/drm/i915/i915_gem.c b/drivers/gpu/drm/i915/i915_gem.c
index 47cc932..833bfad6 100644
--- a/drivers/gpu/drm/i915/i915_gem.c
+++ b/drivers/gpu/drm/i915/i915_gem.c
@@ -4869,7 +4869,7 @@ static void __i915_gem_free_objects(struct drm_i915_private *i915,
 		if (obj->ops->release)
 			obj->ops->release(obj);
 
-		if (WARN_ON(i915_gem_object_has_pinned_pages(obj)))
+		if (i915_gem_object_has_pinned_pages(obj))
 			atomic_set(&obj->mm.pages_pin_count, 0);
 		__i915_gem_object_put_pages(obj, I915_MM_NORMAL);
 		GEM_BUG_ON(i915_gem_object_has_pages(obj));
diff --git a/drivers/gpu/drm/i915/i915_gem_vgtbuffer.c b/drivers/gpu/drm/i915/i915_gem_vgtbuffer.c
new file mode 100644
index 00000000..b5017c6
--- /dev/null
+++ b/drivers/gpu/drm/i915/i915_gem_vgtbuffer.c
@@ -0,0 +1,326 @@
+/*
+ * Copyright Â© 2012 - 2015 Intel Corporation
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a
+ * copy of this software and associated documentation files (the "Software"),
+ * to deal in the Software without restriction, including without limitation
+ * the rights to use, copy, modify, merge, publish, distribute, sublicense,
+ * and/or sell copies of the Software, and to permit persons to whom the
+ * Software is furnished to do so, subject to the following conditions:
+ *
+ * The above copyright notice and this permission notice (including the next
+ * paragraph) shall be included in all copies or substantial portions of the
+ * Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL
+ * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ *
+ */
+
+#include "i915_drv.h"
+#include "intel_drv.h"
+
+#include "gvt/gvt.h"
+
+static int i915_gem_vgtbuffer_get_pages(struct drm_i915_gem_object *obj)
+{
+	BUG();
+	return -EINVAL;
+}
+
+static void i915_gem_vgtbuffer_put_pages(struct drm_i915_gem_object *obj,
+					 struct sg_table *st)
+{
+	/* like stolen memory, this should only be called during free
+	 * after clearing pin count.
+	 */
+	sg_free_table(st);
+	kfree(st);
+}
+
+static const struct drm_i915_gem_object_ops i915_gem_vgtbuffer_ops = {
+	.get_pages = i915_gem_vgtbuffer_get_pages,
+	.put_pages = i915_gem_vgtbuffer_put_pages,
+};
+
+#define GEN8_DECODE_PTE(pte) \
+	((dma_addr_t)(((((u64)pte) >> 12) & 0x7ffffffULL) << 12))
+
+#define GEN7_DECODE_PTE(pte) \
+	((dma_addr_t)(((((u64)pte) & 0x7f0) << 28) | (u64)(pte & 0xfffff000)))
+
+static struct sg_table *
+i915_create_sg_pages_for_vgtbuffer(struct drm_device *dev,
+				   u32 start, u32 num_pages)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct sg_table *st;
+	struct scatterlist *sg;
+	int i;
+
+	st = kmalloc(sizeof(*st), GFP_KERNEL);
+	if (st == NULL)
+		return NULL;
+
+	if (sg_alloc_table(st, num_pages, GFP_KERNEL)) {
+		kfree(st);
+		return NULL;
+	}
+
+	if (INTEL_GEN(dev_priv) >= 8) {
+		gen8_pte_t __iomem *gtt_entries =
+			(gen8_pte_t __iomem *)dev_priv->ggtt.gsm +
+			(start >> PAGE_SHIFT);
+		for_each_sg(st->sgl, sg, num_pages, i) {
+			sg->offset = 0;
+			sg->length = PAGE_SIZE;
+			sg_dma_address(sg) =
+				GEN8_DECODE_PTE(readq(&gtt_entries[i]));
+			sg_dma_len(sg) = PAGE_SIZE;
+		}
+	} else {
+		gen6_pte_t __iomem *gtt_entries =
+			(gen6_pte_t __iomem *)dev_priv->ggtt.gsm +
+			(start >> PAGE_SHIFT);
+		for_each_sg(st->sgl, sg, num_pages, i) {
+			sg->offset = 0;
+			sg->length = PAGE_SIZE;
+			sg_dma_address(sg) =
+				GEN7_DECODE_PTE(readq(&gtt_entries[i]));
+			sg_dma_len(sg) = PAGE_SIZE;
+		}
+	}
+
+	return st;
+}
+
+struct drm_i915_gem_object *
+i915_gem_object_create_vgtbuffer(struct drm_device *dev,
+				 u32 start, u32 num_pages)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct drm_i915_gem_object *obj;
+	struct sg_table *st;
+
+	obj = i915_gem_object_alloc(dev_priv);
+	if (obj == NULL)
+		return NULL;
+
+	drm_gem_private_object_init(dev, &obj->base, num_pages << PAGE_SHIFT);
+	i915_gem_object_init(obj, &i915_gem_vgtbuffer_ops);
+
+	st = i915_create_sg_pages_for_vgtbuffer(dev, start, num_pages);
+	if (st == NULL) {
+		i915_gem_object_free(obj);
+		return NULL;
+	}
+
+	__i915_gem_object_set_pages(obj, st, num_pages << PAGE_SHIFT);
+
+	if (i915_gem_object_pin_pages(obj)) {
+		i915_gem_object_free(obj);
+		return NULL;
+	}
+
+	i915_gem_object_set_cache_coherency(obj, I915_CACHE_LLC);
+
+	DRM_DEBUG_DRIVER("VGT_GEM: backing store base = 0x%x pages = 0x%x\n",
+			 start, num_pages);
+	return obj;
+}
+
+static u8 vgt_get_tiling_mode(struct drm_i915_private *dev_priv, u32 tiling)
+{
+	u8 tiling_mode = I915_TILING_NONE;
+
+	if (IS_HASWELL(dev_priv) || IS_BROADWELL(dev_priv))
+		tiling_mode = (tiling ? I915_TILING_X : I915_TILING_NONE);
+	else if (INTEL_GEN(dev_priv) >= 9) {
+		switch (tiling) {
+		case PLANE_CTL_TILED_LINEAR:
+			tiling_mode = I915_TILING_NONE;
+			break;
+		case PLANE_CTL_TILED_X:
+			tiling_mode = I915_TILING_X;
+			break;
+		case PLANE_CTL_TILED_Y:
+		case PLANE_CTL_TILED_YF:
+			tiling_mode = I915_TILING_Y;
+			break;
+		default:
+			DRM_DEBUG_DRIVER("VGT_GEM: unsupported tile format:%x\n",
+					 tiling);
+		}
+	} else
+		DRM_DEBUG_DRIVER("VGT_GEM: unsupported platform!\n");
+
+	return tiling_mode;
+}
+
+static int vgt_decode_information(struct drm_device *dev,
+				  struct intel_vgpu *vgpu,
+				  struct drm_i915_gem_vgtbuffer *args)
+{
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_vgpu_primary_plane_format p;
+	struct intel_vgpu_cursor_plane_format c;
+	u32 gtt_fbstart;
+	int i, ret;
+	u64 gtt_hash = 0;
+	void __iomem *gtt_base = dev_priv->ggtt.gsm; /* mappable_base */
+
+	if (args->plane_id == I915_VGT_PLANE_PRIMARY) {
+		ret = intel_vgpu_decode_primary_plane(vgpu, &p);
+		if (ret)
+			return ret;
+
+		args->enabled = p.enabled;
+		args->x_offset = p.x_offset;
+		args->y_offset = p.y_offset;
+		args->start = p.base;
+		args->width = p.width;
+		args->height = p.height;
+		args->stride = p.stride;
+		args->bpp = p.bpp;
+		args->hw_format = p.hw_format;
+		args->drm_format = p.drm_format;
+		args->tiled = vgt_get_tiling_mode(dev_priv, p.tiled);
+		args->size = (p.stride * roundup(p.height, (args->tiled > I915_TILING_X) ? 32 : 8) +
+			      (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+	} else if (args->plane_id == I915_VGT_PLANE_CURSOR) {
+		ret = intel_vgpu_decode_cursor_plane(vgpu, &c);
+		if (ret)
+			return ret;
+
+		args->enabled = c.enabled;
+		args->x_offset = c.x_hot;
+		args->y_offset = c.y_hot;
+		args->x_pos = c.x_pos;
+		args->y_pos = c.y_pos;
+		args->start = c.base;
+		args->width = c.width;
+		args->height = c.height;
+		args->stride = c.width * (c.bpp / 8);
+		args->bpp = c.bpp;
+		args->tiled = 0;
+		args->size = (((c.width * c.height * c.bpp) / 8) +
+			      (PAGE_SIZE - 1)) >> PAGE_SHIFT;
+	} else {
+		DRM_DEBUG_DRIVER("VGT_GEM: Invalid plaine_id: %d\n",
+				 args->plane_id);
+		return -EINVAL;
+	}
+
+	if (args->start & (PAGE_SIZE - 1)) {
+		DRM_DEBUG_DRIVER("VGT_GEM: Not aligned fb start address: "
+				 "0x%x\n", args->start);
+		return -EINVAL;
+	}
+
+	if (((args->start >> PAGE_SHIFT) + args->size) >
+	    ggtt_total_entries(&dev_priv->ggtt)) {
+		DRM_DEBUG_DRIVER("VGT_GEM: Invalid GTT offset or size\n");
+		return -EINVAL;
+	}
+
+	DRM_DEBUG_DRIVER("VGT_GEM: Surface size = %d\n",
+			 (int)(args->size * PAGE_SIZE));
+
+	gtt_fbstart = args->start >> PAGE_SHIFT;
+
+	DRM_DEBUG_DRIVER("VGT_GEM: gtt start addr %p\n", gtt_base);
+	DRM_DEBUG_DRIVER("VGT_GEM: fb start %x\n", gtt_fbstart);
+
+	if (INTEL_GEN(dev_priv) >= 8)
+		gtt_base = (gen8_pte_t __iomem *)gtt_base + gtt_fbstart;
+	else
+		gtt_base = (gen6_pte_t __iomem *)gtt_base + gtt_fbstart;
+
+	DRM_DEBUG_DRIVER("VGT_GEM: gtt + fb start %p\n", gtt_base);
+
+	for (i = 0; i < args->size; i++) {
+		u64 gtt_pte, overflow;
+
+		if (INTEL_GEN(dev_priv) >= 8)
+			gtt_pte = readq((gen8_pte_t __iomem *)gtt_base + i);
+		else
+			gtt_pte = readl((gen8_pte_t __iomem *)gtt_base + i);
+
+		gtt_hash = (gtt_hash << 4) + gtt_pte;
+		overflow = gtt_hash & (0xffful << 32);
+		if (overflow != 0) {
+			gtt_hash ^= overflow >> 32;
+			gtt_hash ^= overflow;
+		}
+	}
+
+	DRM_DEBUG_DRIVER("VGT_GEM: gtt_hash=0x%lx\n", (unsigned long)gtt_hash);
+	args->hash = gtt_hash;
+
+	return 0;
+}
+
+/**
+ * Creates a new mm object that wraps some user memory.
+ */
+int i915_gem_vgtbuffer_ioctl(struct drm_device *dev, void *data,
+			     struct drm_file *file)
+{
+	struct drm_i915_gem_vgtbuffer *args = data;
+	struct drm_i915_private *dev_priv = dev->dev_private;
+	struct intel_vgpu *vgpu;
+	struct drm_i915_gem_object *obj;
+	u32 handle;
+	int ret;
+
+	if (INTEL_GEN(dev_priv) < 7) {
+		DRM_DEBUG_DRIVER("VGT_GEM: Unsupported architecture\n");
+		return -EPERM;
+	}
+
+	if (!intel_gvt_active(dev_priv)) {
+		DRM_DEBUG_DRIVER("VGT_GEM: GVT-g is not active\n");
+		return -EPERM;
+	}
+
+	vgpu = intel_gvt_hypervisor_vgpu_from_vm_id(args->vmid);
+	if (!vgpu) {
+		DRM_DEBUG_DRIVER("VGT_GEM: No vgpu found for a given VM id\n");
+		return -EINVAL;
+	}
+
+	ret = vgt_decode_information(dev, vgpu, args);
+	if (ret)
+		return ret;
+
+	if (args->flags & I915_VGTBUFFER_QUERY_ONLY)
+		return 0;
+
+	obj = i915_gem_object_create_vgtbuffer(dev, args->start, args->size);
+	if (!obj) {
+		DRM_DEBUG_DRIVER("VGT_GEM: Failed to create gem object"
+				 " for VM FB!\n");
+		return -EINVAL;
+	}
+
+	obj->tiling_and_stride = args->tiled | args->stride;
+
+	ret = drm_gem_handle_create(file, &obj->base, &handle);
+	if (ret) {
+		/* TODO: Double confirm the error handling path */
+		i915_gem_object_unpin_pages(obj);
+		i915_gem_object_free(obj);
+		return ret;
+	}
+
+	/* drop reference from allocate - handle holds it now */
+	drm_gem_object_unreference_unlocked(&obj->base);
+
+	args->handle = handle;
+	return 0;
+}
diff --git a/include/uapi/drm/i915_drm.h b/include/uapi/drm/i915_drm.h
index 7f5634c..0b18887 100644
--- a/include/uapi/drm/i915_drm.h
+++ b/include/uapi/drm/i915_drm.h
@@ -319,6 +319,7 @@ typedef struct _drm_i915_sarea {
 #define DRM_I915_PERF_ADD_CONFIG	0x37
 #define DRM_I915_PERF_REMOVE_CONFIG	0x38
 #define DRM_I915_QUERY			0x39
+#define DRM_I915_GEM_VGTBUFFER		0x40
 
 #define DRM_IOCTL_I915_INIT		DRM_IOW( DRM_COMMAND_BASE + DRM_I915_INIT, drm_i915_init_t)
 #define DRM_IOCTL_I915_FLUSH		DRM_IO ( DRM_COMMAND_BASE + DRM_I915_FLUSH)
@@ -377,6 +378,7 @@ typedef struct _drm_i915_sarea {
 #define DRM_IOCTL_I915_PERF_ADD_CONFIG	DRM_IOW(DRM_COMMAND_BASE + DRM_I915_PERF_ADD_CONFIG, struct drm_i915_perf_oa_config)
 #define DRM_IOCTL_I915_PERF_REMOVE_CONFIG	DRM_IOW(DRM_COMMAND_BASE + DRM_I915_PERF_REMOVE_CONFIG, __u64)
 #define DRM_IOCTL_I915_QUERY			DRM_IOWR(DRM_COMMAND_BASE + DRM_I915_QUERY, struct drm_i915_query)
+#define DRM_IOCTL_I915_GEM_VGTBUFFER		DRM_IOWR(DRM_COMMAND_BASE + DRM_I915_GEM_VGTBUFFER, struct drm_i915_gem_vgtbuffer)
 
 /* Allow drivers to submit batchbuffers directly to hardware, relying
  * on the security mechanisms provided by hardware.
@@ -1717,6 +1719,43 @@ struct drm_i915_query_topology_info {
 	__u8 data[];
 };
 
+struct drm_i915_gem_vgtbuffer {
+	__u32 vmid;
+	__u32 plane_id;
+#define I915_VGT_PLANE_PRIMARY 1
+#define I915_VGT_PLANE_SPRITE 2
+#define I915_VGT_PLANE_CURSOR 3
+	__u32 pipe_id;
+	__u32 phys_pipe_id;
+	__u8  enabled;
+	__u8  tiled;
+	__u32 bpp;
+	__u32 hw_format;
+	__u32 drm_format;
+	__u32 start;
+	__u32 x_pos;
+	__u32 y_pos;
+	__u32 x_offset;
+	__u32 y_offset;
+	__u32 size;
+	__u32 width;
+	__u32 height;
+	__u32 stride;
+	__u64 user_ptr;
+	__u32 user_size;
+	__u32 flags;
+#define I915_VGTBUFFER_READ_ONLY (1<<0)
+#define I915_VGTBUFFER_QUERY_ONLY (1<<1)
+#define I915_VGTBUFFER_UNSYNCHRONIZED 0x80000000
+	__u32 hash;
+	/**
+	 * Returned handle for the object.
+	 *
+	 * Object handles are nonzero.
+	 */
+	__u32 handle;
+};
+
 #if defined(__cplusplus)
 }
 #endif
